        \section{Differentiation of fields}
        One may now wonder where the calculus comes in.  We are almost there, but we should know what it means intuitively to use the calculus tools with these more general fields.
        
        Curves will prove to be a great tool in the analysis of these scalar fields.  We will also need to understand how vectors transform from point to point which will require us to recall the knowledge we gained on linear transformations.
        
        Roughly speaking, we will have:
        \begin{itemize}
            \item \textbf{Curves:} Derivatives of curves $\gamma(t)$ are vectors. Specifically, we have already seen the tangent vector $\gamma'(t)$ and acceleration vector $\gamma''(t)$.
            \item \textbf{Scalar Fields:} Derivatives of scalar fields $f(x,y,z)$ will depend on which input of the function we change.  We can collect the partial derivatives corresponding to finding derivatives in one input into a vector called the \emph{gradient}.
            \item \textbf{Vector Fields:} Derivatives of vector fields $\mathbf{v}(x,y,z)$ will require us to see how each component of $\mathbf{v}$ changes based on how each individual input changes.  In this case, this collection of derivatives will be put into a matrix known as the \emph{total derivative}.
        \end{itemize}
        
        \begin{remark}
        The important notion that we need to understand is the following.
        
        \emph{The derivative of a function at a point is the best linear approximation to the function.} 
        
        Based on this wording, it is intuitive to imagine derivatives as vectors or matrices.
        \end{remark}
        
        \section{Functions of Fields}
        In our world, we often care about combining different fields together.  For example, we can take the electric field created by a single charged particle and add this field to another field created by a different charged particle. What I mean, is we can write
        \[
        \mathbf{v}(x,y,z)+\mathbf{u}(x,y,z)
        \]
        and make sense of this.  Just as we did with vectors, we add the components together! That is if we have
        \begin{align*}
        \mathbf{v}(x,y,z)&=(f_1(x,y,z),f_2(x,y,z),f_3(x,y,z))\\ \mathbf{u}(x,y,z)&=(g_1(x,y,z),g_2(x,y,z),g_3(x,y,z)),
        \end{align*}
        then
        \[
        \mathbf{v}(x,y,z)+\mathbf{u}(x,y,z)=(f_1+g_1,f_2+g_2,f_3+g_3).
        \]
        Intuitively, this just adds together the vectors at each point!
        
        \begin{ex}{Addition of vector fields}{add_vect_fields}
        Consider the following vector fields
        \begin{align*}
            \mathbf{v}(x,y)&=(x,x)\\
            \mathbf{u}(x,y)&=(y,y).
        \end{align*}
        These look like:
        \begin{figure}[H]
            \centering
            \begin{subfigure}[h]{.45\textwidth}
            \includegraphics[width=\textwidth]{Figures_Part_6/vec_v.png}
            \caption{Vector field $\mathbf{v}$.}
            \end{subfigure}
            ~
            \begin{subfigure}[h]{.45\textwidth}
            \includegraphics[width=\textwidth]{Figures_Part_6/vec_u.png}
            \caption{Vector field $\mathbf{u}$.}
            \end{subfigure}
        \end{figure}
        Adding these results in the field
        \begin{figure}[H]
            \centering
            \includegraphics[width=.45\textwidth]{Figures_Part_6/v_field_1.png}
            \caption{The vector field $\vecfieldU+\vecfieldV$.}
        \end{figure}
        \end{ex}
        
        \begin{remark}
        If we do this all for vector fields, we can take curves and scalar fields as special cases.
        \begin{itemize}
            \item Adding together scalar fields is the same as adding functions.  They just have more inputs to think about!
            \item Adding together curves is done in the same componentwise manner that we have shown here for vector fields.
        \end{itemize}
        \end{remark}
        
        We can also scale a field by a real number.  This will stretch vectors at each point.
        
        \begin{ex}{Scaling of Vector Fields}{scale_vect_field}
        Just take the vector field
        \[
        \mathbf{v}(x,y,z)=(f_1(x,y,z),f_2(x,y,z),f_3(x,y,z))
        \]
        then we can take
        \[
        \lambda \mathbf{v}=(\lambda f_1, \lambda f_2, \lambda f_3).
        \]
        \end{ex}
        
        \begin{remark}
        There is many reasons why the above is important.  It seems our physical world plays nicely with the above concept, for one.
        
        One thing we can actually do, and will do a bit later, is find that there are two main types of vector fields in $\R^3$.  These will be the curl fields and divergence fields!  This is important in electromagnetism.
        \end{remark}
        
        When we were working with complex numbers, we considered integrating a complex function $f(z)$ over a contour $\gamma(t)$ from $t=a$ to $t=b$.  This involved computing
        \[
        \int_\gamma f(\gamma)d\gamma = \int_a^b f(\gamma(t))\gamma'(t)dt.
        \]
        I introduced this concept since it appears in the context of multivariate functions.  However, in the multivariate case, we needed some more tools.
        
        Now, the idea is as follows.  We will be given a scalar field, $f(x,y,z)$ or a vector field $\mathbf{F}(x,y,z)$ and a curve $\gamma(t)$.  
        
        \section{Line integrals of scalar fields}
        For the purpose of visualization, we will look at scalar fields of two variables and curves in the plane.  Our set up will have $f(x,y)$ and $\gamma(t)$ over the time $t=a$ to $t=b$.
        
        We want to understand the following:
        \[
        \int_\gamma f(\gamma)d\gamma \coloneqq \int_a^b f(\gamma(t))\|\gamma'(t)\|dt.
        \]
        Intuitively speaking, this integral finds the area under the curve $\gamma$ along the graph of $f$.  This is analogous to what we did in one dimension! See the following figure.
        \begin{figure}[H]
            \centering
            \includegraphics[width=.5\textwidth]{Figures_Part_6/Line_integral_of_scalar_field.jpg}
        \end{figure}
        
        \begin{ex}{Length of a curve}{length_of_curve}
        Let $f(x,y,z)=1$ and $\gamma(t)$ be a curve over $t=a$ to $t=b$.  Then the line integral
        \[
        \int_\gamma f(\gamma)d\gamma = \int_a^b \|\gamma'(t)\|dt
        \]
        is known as the \emph{length} (sometimes \emph{arclength} of the curve $\gamma$.
        \end{ex}
        
        \begin{ex}{Line Integral on a Paraboloid}{line_int_parabooid}
        Consider the function $f(x,y)=x^2+y^2$ and $\gamma(t)=(t,t)$ over $t=0$ to $t=1$.  Then the line integral
        \[
        \int_\gamma f(\gamma)d\gamma = \int_0^1 f(\gamma(t))\|\gamma'(t)\|dt.
        \]
        We have
        \begin{itemize}
            \item $f(\gamma(t))=t^2+t^2=2t^2.$
            \item $\|\gamma'(t)\|=\|(1,1)\|=\sqrt{2}.$
        \end{itemize}
        So we have
        \[
        \int_\gamma f(\gamma)d\gamma = \int_0^1 2\sqrt{2}t^2dt.
        \]
        This evaluates to $\frac{2\sqrt{2}}{3}$.
        \end{ex}
        
        \begin{exercise}
        Integrate $f(x,y)=x+y$ along the curve $\gamma(t)=(t,0)$ from $t=0$ to $t=1$. What do you notice about this? Can we tie this to one-dimensional integration?
        \end{exercise}
        
        \section{Line integrals of vector fields}
        
        There is also a type of line integral that works alongside vector fields.  Roughly, the idea is to add up how much a vector field is pointing along the curve throughout the length of the curve.  
        
        Here we are given $\mathbf{F}(x,y,z)$ is a vector field, $\gamma(t)$ is a curve over the time $t=a$ to $t=b$.  Then we can write
        \[
        \int_\gamma \mathbf{F}(\gamma)\cdot d\gamma =\int_a^b \mathbf{F}(\gamma(t))\cdot \gamma'(t) dt.
        \]
        
        \begin{ex}{Work Done on a Particle}{work_on_particle}
        The work done on a particle (or change in energy) is written as a line integral of this form.  
        
        Take for example, $\mathbf{F}(x,y)=(2x,3y)$ and $\gamma(t)=(t,t^2)$ over the time $t=0$ to $t=1$.  Then
        \begin{align*}
        \int_\gamma \mathbf{F}(\gamma)\cdot d\gamma &= \int_0^1 \mathbf{F}(t,t^2)\cdot (t,t^2)dt\\
        &= \int_0^1 (2t,2t^2)\cdot(t,t^2)dt\\
        &=\int_0^1 2t^2+2t^4dt.
        \end{align*}
        Which I'll leave to you to evaluate if you'd like.
        \end{ex}
        
        \begin{remark}
        This notion is extremely important in defining something called the \emph{potential} of a vector field.  This will show up in electrodynamics.
        
        If the force in the example above is \emph{conservative}, we will have a potential.  This will correspond nicely to the vector field having no \emph{curl}.
        \end{remark}
        
        \section{Derivatives}
        Having done some integral calculus, it's time to head back to differential calculus.  We should say the following. This is far more abstract than we need, but it is an important realization.  
        
        \begin{df}{The Derivative}{derivative}
        Given a function $f\colon \R^n \to \R^m$, the \textbf{derivative of $f$ at the point $\mathbf{x}_0$} is the best \emph{linear approximation} to $f$ at the point $\mathbf{x}_0$.
        
        What this means is the following: If we zoom in to the point $\mathbf{x}_0$ and see what $f$ does in this region, we'll notice that $f$ is approximately a linear transformation.
        \end{df}
        
        Now, let us review the one-dimensional derivative that we are familiar with.
        
        \begin{df}{Derivative on $\R$}{der_on_R}
        Given $f\colon \R \to \R$, we define the \textbf{derivative $f'$ of $f$ at the point $x_0$} by
        \[
        f'(x_0)\coloneqq \lim_{\delta \to 0} \frac{f(x_0+\delta)-f(x_0)}{\delta}.
        \]
        This value $f'(x_0)$ is a number, and moreover is a $1\times 1$-matrix!  We often hide this idea at first.
        \end{df}
        
        It's a bit silly to say $f'(x_0)$ is a $1\times 1$-matrix, but in the end it will help us to remember this.
        
        \subsubsection{Derivatives of scalar fields}
        We can investigate functions of multiple variables in more ways than the single variable case.  Let us start with scalar fields.
        
        \begin{df}{Partial Derivatives}{partial_derivs}
        Let $f\colon \R^3 \to \R$ be a scalar field.  Then we can consider derivatives with respect to changing each input.  For example, we define the \textbf{partial derivative with respect to $x$} \textbf{at the point $(x_0,y_0,z_0)$}, denoted $\frac{\partial f}{\partial x}$, and put
        \[
        \frac{\partial f}{\partial x}(x_0,y_0,z_0)\coloneqq \lim_{\delta \to 0} \frac{f(x_0+\delta,y_0,z_0)-f(x_0,y_0,z_0)}{\delta}
        \]
        \end{df}
        
        \begin{remark}
        For partial derivatives, all but one variable are being held constant.  So, when you are computing these, be sure to treat the proper variables as constant when necessary.
        \end{remark}
        
        \begin{exercise}
        Define $\frac{\partial f}{\partial y}$ and $\frac{\partial f}{\partial z}$ in a similar way to the above definition.
        \end{exercise}
        
        \begin{exercise}
        Compute $\partialx$, $\partialy$, and $\partialz$ for the function 
        \[
        f(x,y,z)=\sin(xyz)+x+2y^2+3x^2z.
        \]
        \end{exercise}
        
        It turns out that collecting the partial derivatives as a vector is the best linear approximation to a scalar function.  We call this vector the gradient vector.
        
        \begin{df}{The Gradient}{gradient}
        Given a scalar field $f(x,y,z)$, the \textbf{gradient of $f$ at the point $(x_0,y_0,z_0)$}, denoted $\nabla f(x_0,y_0,z_0)$ is given by
        \[
        \nabla f(x_0,y_0,z_0)=\begin{bmatrix} \partialx(x_0,y_0,z_0)\\ \partialy(x_0,y_0,z_0)\\ \partialz(x_0,y_0,z_0)\end{bmatrix}.
        \]
        \end{df}
        
        \begin{exercise}
        Compute the gradient for the function
        \[
        f(x,y,z)=\sin(xyz)+x+2y^2+3x^2z.
        \]
        \end{exercise}
        
        \subsubsection{Properties of partial derivatives and the gradient}
        
        As with the one-dimensional derivative, we have some properties that will be helpful.\\
        
        \noindent\textbf{Partial Derivatives:}
        \begin{enumerate}[(i)]
            \item \textbf{Sum Rule:} Given $f(x,y,z)$ and $g(x,y,z)$, we have that
            \[
            \frac{\partial}{\partial x} (f(x,y,z)+g(x,y,z))=\partialx + \frac{\partial g}{\partial x}.
            \]
            Of course, this holds for any partial derivative.
            \item \textbf{Constant Multiple:} Given $\lambda \in \R$ and $f(x,y,z)$, we have that
            \[
            \frac{\partial}{\partial x}(\lambda f(x,y,z))=\lambda \partialx.
            \]
            Again, this holds for any partial derivative.
            \item \textbf{Product Rule:} Given $f(x,y,z)$ and $g(x,y,z)$ we have that
            \[
            \frac{\partial }{\partial x}(f(x,y,z)g(x,y,z))=\partialx g + f \frac{\partial g}{\partial x}.
            \]
            Ths holds for all partial derivatives.
        \end{enumerate}
        
        \begin{remark}
        The chain rule will show up eventually, but not yet.  As for the quotient rule, this also holds, but I don't show it here.
        \end{remark}
        
        \noindent\textbf{The Gradient:}
        \begin{enumerate}[(i)]
            \item \textbf{Sum Rule:} Given $f(x,y,z)$ and $g(x,y,z)$, we have that
            \[
            \nabla(f(x,y,z)+g(x,y,z))=\nabla f(x,y,z)+\nabla g(x,y,z).
            \]
            \item \textbf{Constant Multiple:} Given $\lambda \in \R$ and $f(x,y,z)$, we have that
            \[
            \nabla(\lambda f(x,y,z))=\lambda \nabla f(x,y,z).
            \]
            \item \textbf{Product Rule:} Given $f(x,y,z)$ and $g(x,y,z)$ we have that
            \[
            \nabla(f(x,y,z)g(x,y,z))=(\nabla f(x,y,z))g(x,y,z)+f(x,y,z)(\nabla g(x,y,z))
            \]
            
        \end{enumerate}
        
        We've learned how to compute partial derivatives and the gradient, but what are they really telling us? Remember that the derivative $\frac{d}{dx}$ of a function $f(x)$ tells us the rate of change of $f$ as we move in the $x$-direction.  This is very similar to what $\frac{\partial}{\partial x}$ tells us about a function $f(x,y,z)$.  So we can say the following.
        \begin{itemize}
            \item $\frac{\partial f}{\partial x}$ tells us how $f$ changes as we move in the $x$-direction.
            \item $\frac{\partial f}{\partial y}$ tells us how $f$ changes as we move in the $y$-direction.
            \item $\frac{\partial f}{\partial z}$ tells us how $f$ changes as we move in the $z$-direction.
        \end{itemize}
        
        We can put these together into the gradient $\nabla f$ and know how $f$ changes in each possible direction. Let's see how the gradient acts then. 
        
        \begin{ex}{Gradients on the Paraboloid}
        Let us start with $f(x,y)=x^2+y^2$.  Then
        \[
        \nabla f(x,y) = (2x,2y).
        \]
        Let us plot the level curves of this surface.
        \begin{figure}[H]
            \centering
            \begin{subfigure}[h]{.45\textwidth}
            \includegraphics[width=\textwidth]{Figures_Part_6/level_curves_gradient.png}
            \caption{Level curves of $f(x,y)$.}
            \end{subfigure}
            ~
            \begin{subfigure}[h]{.45\textwidth}
            \includegraphics[width=\textwidth]{Figures_Part_6/level_curves_gradient_vectors.png}
            \caption{Gradient vectors shown in blue.}
            \end{subfigure}
        \end{figure}
        \begin{itemize}
            \item Notice that the gradient vectors point in a direction perpendicular to the level curves and the length corresponds to how close the nearest level curve is.
            \item The gradient is zero at the bottom of this surface.  
        \end{itemize}
        \end{ex}
        
        \begin{prop}{Gradient Points Uphill}{gradient_prop}
        The gradient $\nabla f(x,y,z)$ is the vector that points in the direction of greatest increase for a function $f(x,y,z)$.
        \end{prop}
        
        How about second partial derivatives? What can we say here. We have each of the following for a function $f(x,y)$:
        \begin{itemize}
            \item $\frac{\partial^2 f}{\partial x^2}$
            \item $\frac{\partial^2 f}{\partial y^2}$
            \item $\frac{\partial}{\partial y}\frac{\partial f}{\partial x}$
            \item $\frac{\partial}{\partial x}\frac{\partial f}{\partial y}$
        \end{itemize}
        
        Recall what $\frac{d^2 f}{dx^2}$ meant for a function $f(x)$.  This told us how $f$ was curving (or what concavity $f$ had). The story is similar for these partial derivatives.
        
        \begin{itemize}
            \item $\frac{\partial^2 f}{\partial x^2}$ tells us about the concavity (or curvature) of $f$ as we move in the $x$ direction.
            \item $\frac{\partial^2 f}{\partial y^2}$ tells us about the concavity (or curvature) of $f$ as we move in the $y$ direction.
            \item We actually have that $\frac{\partial}{\partial y}\frac{\partial f}{\partial x}=\frac{\partial}{\partial x}\frac{\partial f}{\partial y}$.  This interpretation is a bit hard to deal with.  Let's not worry too much about it.
        \end{itemize}
        
        \begin{prop}{Partial Derivatives Commute}{partials_commute}
        We have that
        \[
        \frac{\partial}{\partial y}\frac{\partial f}{\partial x}=\frac{\partial}{\partial x}\frac{\partial f}{\partial y}.
        \]
        Moreover, for functions of more variables, we can say that the order we take derivatives does not matter.
        \end{prop}
        
        \begin{exercise}
        Given $f(x,y)=x^2+y^2$, compute
        \[
        \frac{\partial^2 f}{\partial x^2}, ~ \frac{\partial^2 f}{\partial y^2}, ~ \frac{\partial}{\partial y}\frac{\partial f}{\partial x},~ \frac{\partial}{\partial x}\frac{\partial f}{\partial y}.
        \]
        What can we say about the curvature of $f$ in the two directions? Does this make sense?
        \end{exercise}
        
        \section{Optimization}
        In single variable calculus, we optimized functions $f(x)$ by finding the point $x_0$ where 
        \[
        f'(x_0)=0.
        \]
        We called this a \emph{critical point}. We found if this optimizer $x_0$ was a maximizer or minimizer by checking the sign of second derivative $f''(x_0)$. We had
        \begin{align*}
            \textrm{Maximum: }& f''(x_0)<0\\
            \textrm{Minimum: }& f''(x_0)>0.
        \end{align*}
        
        In higher dimensions, this idea works similarly. We just have more to check. 
        
        \begin{df}{Stationary Points}{stationary_points}
        Given a function $f(x,y)$, we call a point $(x_0,y_0)$ a \textbf{stationary point} if 
        \[
        \nabla f(x_0,y_0) = \mathbf{0}.
        \]
        \end{df}
        
        As before, we will use second derivatives to find out whether this is a maximum or a minimum.
        
        \begin{prop}{Maximizers and Minimizers}{max_min}
        A stationary point $(x_0,y_0)$ is a 
        \begin{align*}
            \textrm{Maximizer if~ }& \frac{\partial^2 f}{\partial x^2} <0 \textrm{ ~and~ } \frac{\partial^2 f}{\partial y^2}<0,\\
            \textrm{Minimizer if~ }& \frac{\partial^2 f}{\partial x^2} >0 \textrm{ ~and~ } \frac{\partial^2 f}{\partial y^2}>0,\\
            \textrm{Saddle if otherwise.}
        \end{align*}
        \end{prop}
        
        \begin{exercise}
        Let
        \[
        f(x,y) = \frac{xy}{e^{x^2+y^2}}.
        \]
        \begin{enumerate}[(a)]
            \item Find all stationary points for $f$.
            \item Determine whether these points are minimizers or maximizers.
        \end{enumerate}
        \end{exercise}
        
        \begin{exercise}
        Show that $f(x,y)=xy$ has a saddle point at $(0,0)$.
        \end{exercise}
        
        \subsubsection{Lagrange Multipliers}
        Often times we are given a situation that we wish to find an optimum solution to, but we are somehow constrained.  A biological example would be fixing a given volume for a red blood cell, and finding the optimum shape so that oxygen diffusion is maximized.  A physical example would be finding the fastest path between two points when moving through a medium with varying viscosity.
        
        The idea is relatively tame. We are given the function to optimize
        \[
        f(x,y,z)
        \]
        and the constraining function
        \[
        g(x,y,z)=k.
        \]
        Then we must solve the equation
        \[
        \nabla f(x,y,z) = \lambda \nabla g (x,y,z)
        \]
        where $\lambda$ is called the \textbf{Lagrange multiplier}. Let's work through an example of solving this.
        
        \begin{ex}{Largest Box}{largest_box}
        Let's find the dimensions of the box with the largest volume if the total surface area is $64 cm^2$.  We must determine our function to optimize, which is the volume function
        \[
        V(x,y,z) = xyz.
        \]
        Our constraint is the surface area function $A(x,y,z)$ must be
        \[
        g(x,y,z)=2xy+2xz+2yz=64,
        \]
        which we will simplify as
        \[
        xy+xz+yz = 32.
        \]
        \begin{enumerate}
            \item We first take
            \[
            \nabla f(x,y,z) = \begin{bmatrix} yz \\ xz \\ xy \end{bmatrix}
            \]
            and
            \[
            \nabla g(x,y,z) = \begin{bmatrix} 2y+2z \\ 2x+2z \\ 2x+2y\end{bmatrix}.
            \]
            \item This gives us four equations to solve. Three are from the equation $\nabla f(x,y,z) =\nabla g(x,y,z)$
            \begin{align}
                yz &= \lambda (y+z)\\
                xz &= \lambda (x+z)\\
                xy &= \lambda (x+y),
            \end{align}
            and one is from the constraint $g(x,y,z)=64$
            \[
            xy+xz+yz = 32.
            \]
            \item Working through solving these can be nontrivial.  In this case, we can do the following: Multiply (1) by $x$, (2) by $y$, and $(3)$ by $z$.  Which gives us
            \begin{align*}
                xyz &= \lambda x(y+z)\\
                xyz &= \lambda y(x+z)\\
                xyz &= \lambda z(x+y)
            \end{align*}
            Now we can set the first two equal to find
            \[
            \lambda x(y+z)=\lambda y(x+z)
            \]
            which simplifies to 
            \[
            \lambda (xz-yz)=0
            \]
            meaning that $\lambda =0$ or $xz=yz$. Note that $\lambda =0$ is not possible since that will end up giving us zero surface area and we won't satisfy the constraint.
            
            Now $xz-yz=0$ means that $x=y$, which we can substitute back into the equations later. 
            
            We can set the last two equal to find
            \[
            \lambda y (x+z)=\lambda z(x+y)
            \]
            which with similar work tells us $z=y$.  So now we make note of $x=y$ and we have $x=y=z$.  
            
            So now we use the constraint equation with $x=y=z$ and find that we have
            \[
            x^2+y^2+z^2 = 3x^2 = 32
            \]
            which means that $x\approx 3.266$.  Thus
            \[
            V(3.266,3.266,3.266)\approx 34.8376
            \]
            is the largest volume.  \emph{This means our ideal solution is a cube!} 
            
        \end{enumerate}
        \end{ex}
        
        \section{Approximation and the Tangent Space}
        Sometimes it is helpful to know what a surface looks like up close.  In this case, the surface is best approximated by a plane.  This is analogous to how you can approximate functions of a single variable by a line.
        
        \begin{exercise}
        Compute the tangent line to $f(x)=2x^2+5$ at the point $x_0=3$.
        \end{exercise}
        
        \subsubsection{Equation for a Plane}
        
        We haven't worked much with planes in space yet, but we have seen surfaces.  In some sense, planes are the easiest surfaces.  They are, after all, linear objects.
        
        \begin{ex}{Plane and Normal}{plane_normal}
        The equation for a plane is given by
        \[
        ax+by+cz+ d = 0.
        \]
        Notice that this is a linear equation.
        
        Then the \emph{normal vector} to the plane is given by 
        \[
        \mathbf{n} = \begin{bmatrix} a \\ b \\ c\end{bmatrix}.
        \]
        We can see a diagram of this here.
        \begin{figure}[H]
            \centering
            \includegraphics[width=.4\textwidth]{Figures_Part_6/plane_n.png}
        \end{figure}
        \end{ex}
        
        Now, if we are given a surface (defined as a level surface or as the graph of a function), we can compute an approximation at a point called the \emph{tangent plane}.
        
        \begin{ex}{Tangent Plane to Paraboloid}
        Consider the function 
        \[
        f(x,y)=-x^2-y^2.
        \]
        Then the graph of the function is given by plotting the points
        \[
        (x,y,f(x,y)).
        \]
        We compute the tangent plane by computing partial derivatives. We take
        \begin{align*}
        \frac{\partial f}{\partial x} &= -2x\\    
        \frac{\partial f}{\partial y} &= -2y.
        \end{align*}
        Then the equation for a tangent plane at the point $(x_0,y_0,f(x_0,y_0))$ is given by
        \[
        z-f(x_0,y_0)=\partialx (x-x_0)+\partialy (y-y_0).
        \]
        So in our case, we have
        \[
        z-(-x_0^2-y_0^2)=-2x_0(x-x_0)-2y_0(y-y_0).
        \]
        Pictorially, it looks as follows (letting $p=(x_0,y_0,f(x_0,y_0))$):
        \begin{figure}[H]
            \centering
            \includegraphics[width=.4\textwidth]{Figures_Part_6/tangent-planes-1.png}
        \end{figure}
                
        \end{ex}
        
        \begin{ex}{Tangent Vectors}{tangent_vectors}
        Another way to understand this is to compute the \emph{tangent vectors} at a point. Take the same function $f(x,y)=-x^2-y^2$ and compute
        \begin{align*}
            \frac{\partial}{\partial x}\begin{bmatrix} x \\ y \\ f(x,y)\end{bmatrix} &= \begin{bmatrix} 1\\ 0 \\ -2x\end{bmatrix}\\
            \frac{\partial}{\partial y}\begin{bmatrix} x \\ y \\ f(x,y)\end{bmatrix} &= \begin{bmatrix} 0\\ 1 \\ -2y\end{bmatrix}.
        \end{align*}
        Then take the cross product of these two vectors to get the normal vector to the tangent plane
        \[
        \begin{bmatrix} 1 \\ 0 \\-2x\end{bmatrix} \times \begin{bmatrix} 0 \\ 1 \\ -2y\end{bmatrix} = \begin{bmatrix} 2x \\ 2y \\ 1\end{bmatrix}.
        \]
        Pick a point $(x_0,y_0)$ and the normal to tangent plane is given by
        \[
        \mathbf{n}=\begin{bmatrix} 2x_0 \\ 2y_0 \\ 1 \end{bmatrix}.
        \]
        Which leads us to the following equation of a plane (but not exactly the tangent plane)
        \[
        2x_0 x + 2y_0y+z=0.
        \]
        This plane is parallel to the tangent plane and is often a nicer tool.
        \end{ex}
        
        \begin{exercise}
        Take the two equations for the planes above and simplify each to having a zero right hand side.  Then subtract each of these equations from each other and see what the difference is.
        \end{exercise}
        
        
%% START VEC FIELD
        \section{The Jacobian of a Vector Field}
        Recall we can define a three dimensional vector field $\mathbf{v}(x,y,z)$ by
        \[
        \mathbf{v}(x,y,z)=(v_1(x,y,z),v_2(x,y,z),v_3(x,y,z)) = \begin{bmatrix} v_1(x,y,z) \\ v_2(x,y,z) \\ v_3(x,y,z) \end{bmatrix}
        \]
        where we call each $v_1$, $v_2$, and $v_3$ the component functions.  Notice that each component function is a scalar function!
        
        Since each component function is a scalar function, we know how to compute the derivative of each by computing the gradient.  This gives us a way to then talk about the derivative of the vector field as a whole
        
        \begin{df}{Jacobian}{jacobian}
        The \textbf{Jacobian} of a vector field $\mathbf{v}(x,y,z)$ is a matrix
        \[
        J(x,y,z)\coloneqq \begin{bmatrix} \nabla v_1^T \\ \nabla v_2^T \\ \nabla v_3^T \end{bmatrix},
        \]
        where the gradients are transposed (the superscript $T$) so they are are written as row vectors and placed in a matrix. More specifically, we can write this matrix as 
        \[
        J(x,y,z) = \begin{bmatrix} \frac{\partial v_1}{\partial x} & \frac{\partial v_1}{\partial y} & \frac{\partial v_1}{\partial z}\\ \frac{\partial v_2}{\partial x} & \frac{\partial v_2}{\partial y} & \frac{\partial v_2}{\partial z} \\ \frac{\partial v_3}{\partial x} & \frac{\partial v_3}{\partial y} & \frac{\partial v_3}{\partial z} \end{bmatrix}.
        \]
        \end{df}
        
        The Jacobian contains a lot of information. Intuitively, it tells us how each component of the vector field changes in each direction.  
        
        \begin{ex}{Computing the Jacobian, 1}{computing_jacobian}
        Let us consider the vector field
        \[
        \mathbf{v}(x,y,z) = (x^2+y^2,z,x+y+z).
        \]
        Then we can write
        \begin{align*}
            v_1(x,y,z)&= x^2+y^2\\
            v_2(x,y,z)&= z\\
            v_3(x,y,z)&= x+y+z.
        \end{align*}
        So we compute the gradients of each
        \begin{align*}
            \nabla v_1 &= \begin{bmatrix} 2x \\ 2y \\ 0 \end{bmatrix}\\
            \nabla v_2 &= \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}\\
            \nabla v_3 &= \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}.
        \end{align*}
        So then the Jacobian is
        \[as
        J(x,y,z) = \begin{bmatrix} 2x & 2y & 0 \\ 0 & 0 & 1\\ 1 & 1 & 1 \end{bmatrix}.
        \]
        \end{ex}
        
        A related and important quantity comes in the form of the determinant of the Jacobian.  We've previously talked about the determinant of a matrix as telling us the (signed) scaling of volume of a linear transformation.  That is, for example, if we have
        \[
        A = \begin{bmatrix} 2 & 0 & 0\\ 0 & 2 & 0\\ 0 & 0 & 2 \end{bmatrix}
        \]
        then $\det(A)=8$ and we say that volumes of parallelopipeds are increased by a factor of $8$ in this case.  
        
        When you are given a vector field, you can think of different regions of space being stretched differently.  This is why we have that the Jacobian is a matrix that depends on the position $(x,y,z)$.  In this case, the volumes that are being stretched are very very tiny parallelopipeds.  You can think of cubes with side lengths $dx$, $dy$, and $dz$. 
        
        \begin{ex}{Computing the Jacobian, 2}
        We found that given
        \[
        \mathbf{v}(x,y,z) = (x^2+y^2,z,x+y+z)
        \]
        that
        \[
        J(x,y,z) = \begin{bmatrix} 2x & 2y & 0 \\ 0 & 0 & 1 \\ 1 & 1 & 1 \end{bmatrix}
        \]
        As we can see, this matrix depends on position.  Let's compute the determinant
        \[
        |J(x,y,z)|=\det(J(x,y,z))=2(y-x). 
        \]
        Something weird seems to happen with $y=x$ as the determinant $|J(x,y,z)|$ will be zero in this case. Otherwise, things are fine.
        \end{ex}
        
        \section{Divergence and Curl}
        Often we do not need to use the whole Jacobian.  We will find it to be necessary for integration, however.  
        
        For analysis of vector fields, we often wish to break them into their smaller pieces.  Fundamentally, we can break vector fields into two parts:
        \begin{itemize}
            \item Sources and sinks,
            \item Rotations.
        \end{itemize}
        
        \subsubsection{Sources, Sinks, and Divergence Fields}
        With some vector fields, we can make the analogy that some quantity (think air or water) is being added or removed from the system.  We call these \emph{sources} and \emph{sinks} respectively.  We want to quantify how much of some quantity is being added. This quantity is called the \emph{divergence}.
        
        Recall, we can write
        \[
        \nabla = \begin{bmatrix} \frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \\ \frac{\partial}{\partial z} \end{bmatrix}.
        \]
        If we are also given a vector field
        \[
        \mathbf{v}(x,y,z) = \begin{bmatrix} v_1(x,y,z) \\ v_2(x,y,z) \\ v_3(x,y,z) \end{bmatrix},
        \]
        we can compute the \textbf{divergence} of $\mathbf{v}$ by
        \[
        \nabla \cdot \mathbf{v}(x,y,z) = \frac{\partial v_1}{\partial x} + \frac{\partial v_2}{\partial y} + \frac{\partial v_3}{\partial z}.
        \]
        Notice that this quantity is a scalar!  This scalar value, the divergence, tells us how much the vector field is diverging at a point $(x,y,z)$. In other words, it tells us how much of a quanitity is being added or removed there.
        
        \begin{ex}{Source Field}{source_field}
        Consider
        \[
        \mathbf{v}(x,y,z) = \begin{bmatrix} x \\ y \\ z \end{bmatrix}.
        \]
        Then the divergence is
        \[
        \nabla \cdot \mathbf{v}(x,y,z) = \frac{\partial v_1}{\partial x} + \frac{\partial v_2}{\partial y} + \frac{\partial v_3}{\partial z} = 1 + 1 + 1 = 3.
        \]
        We can think of a source of air being placed at each point that pumps in $3$ units of air per second, or specifically being pumped in at the origin. The vector field looks like:
        \begin{figure}[H]
            \centering
            \includegraphics[width=.6\textwidth]{Figures_Part_6/divergence_field.png}
        \end{figure}
        \end{ex}
        
        \subsubsection{Rotation Fields}
        The divergence was the quantity that measured the outflow from a point for a vector field.  The other quantity we can measure is the rotation of a vector field at a point.
        
        We define the \textbf{curl} of a vector field $\mathbf{v}(x,y,z)$ to be
        \[
        \nabla \times \mathbf{v}(x,y,z) = \begin{bmatrix} \frac{\partial v_3}{\partial y} - \frac{\partial v_2}{\partial z} \\ \frac{\partial v_1}{\partial z} - \frac{\partial v_3}{\partial x} \\ \frac{\partial v_2}{\partial x} - \frac{\partial v_1}{\partial y} \end{bmatrix}.
        \]
        \emph{Note that the curl is a vector!} The curl is a vector that points orthogonally to the plane where rotation occurs and has magnitude relative to how quickly the field swirls.
        
        It's a bit involved to go through the work and see exactly why this is the correct quantity for seeing rotation of a vector field.  However, you can recall that the cross product was useful in describing rotational motion of rigid bodies (that is, it showed up in angular velocity/momentum).
        
        \begin{ex}{A Rotation Field}{rot_field}
        Consider the vector field
        \[
        \mathbf{v}(x,y,z) = (-y,x,z)
        \]
        which looks like
        \begin{figure}[H]
            \centering
            \includegraphics[width=.6\textwidth]{Figures_Part_6/curl_field.png}
        \end{figure}
        We let
        \begin{align*}
            v_1(x,y,z) &= -y\\
            v_2(x,y,z) &= x\\
            v_3(x,y,z) &= z.
        \end{align*}
        If we look in this figure where $z=0$, we can clearly see that this field swirls around the origin.  If the curl is to measure rotation, we should see it nonzero here.
        
        Let us compute the curl of this field.  For this, we will all the other partial derivatives not contained in the divergence. That is, we need
        \begin{align*}
            \frac{\partial v_1}{\partial y} &= -1 & \frac{\partial v_1}{\partial z} &= 0\\
            \frac{\partial v_2}{\partial x} &= 1& \frac{\partial v_2}{\partial z} &= 0\\
            \frac{\partial v_3}{\partial x} &= 0 &  \frac{\partial v_3}{\partial y} &=0.
        \end{align*}
        Then we have
        \[
        \nabla \times \mathbf{v}(x,y,z) = \begin{bmatrix} 0-0\\ 0-0 \\ 1-(-1)\end{bmatrix} = \begin{bmatrix} 0\\ 0 \\ 2\end{bmatrix}.
        \]
        
        We can decipher the meaning here by saying that the swirling occurs in planes parallel to the $xy$-plane since the direction of the curl is only in the $z$-direction.  That is, curl is pointing perpendicularly to the plane of rotation.  How quickly the field swirls is given by the magnitude of the curl which is $2$ in this case.  Using the right hand rule we discussed previously this tells us the direction of swirling as well.  We have swirling counter-clockwise in the planes parallel to the $xy$-plane, and so we expect the curl to point in the positive $z$-direction.
        
        Take a moment to analyze this using the figure provided or by plotting this yourself.
        \end{ex}
        
        \begin{ex}{Divergence of the Rotation Field}
        One may also consider the divergent nature of the field 
        \[
        \mathbf{v}(x,y,z) = (-y,x,z)
        \]
        from the previous example and find that 
        \[
        \nabla \cdot \mathbf{v}(x,y,z) = 1.
        \]
        So, there is in some way divergence as well.  This leads us to breaking the vector field into a part that swirls and a part that diverges as follows:
        \begin{align*}
            \mathbf{v}_\textrm{swirl}(x,y,z) &= (-y,x,0)\\
            \mathbf{v}_\textrm{div}(x,y,z) &= (0,0,z).
        \end{align*}
        This type of analysis can be very helpful when considering real world problems.  It is especially important in electromagnetism.
        \end{ex}
        
        \subsubsection{Constant Vector Fields}
        Most of the understanding of vector fields was just covered by understanding the the part that diverges and the part that curls.  However, you can always add constants to these vector fields and these constants will not change the divergence or curl. Why? Take the following example.
        
        \begin{ex}{Constant Fields}{const_field}
        Let 
        \[
        \mathbf{v}(x,y,z) = (c_1,c_2,c_3)
        \]
        where $c_1,c_2,$ and $c_3$ are constants.  Then we can compute the Jacobian of $\mathbf{v}$
        \[
        J(x,y,z) = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0\\ 0 & 0 &0 \end{bmatrix}.
        \]
        Since the Jacobian holds all the partial derivative information, we can know from this that 
        \begin{align*}
            \nabla \cdot \mathbf{v} &= 0\\
            \nabla \times \mathbf{v} &= \mathbf{0}.
        \end{align*}
        \end{ex}
        
        \begin{exercise}
        Specifically show that the divergence and curl of a constant vector field (as in the previous example) are zero.  
        \end{exercise}
        
        All of this is to say that aside from the addition of a constant vector field, we understand the behavior by looking at divergence and curl.
        
        \section{The Laplacian of a Scalar Field}
        In the study of partial differential equations (PDEs), we are often asked to find a function $u(x,y,z)$ that satisfies the following equation
        \[
        \nabla \cdot \nabla u(x,y,z) = f(x,y,z)
        \]
        for some given function $f(x,y,z)$.  We will revisit this Part III, but for now we should see exactly what we mean by
        \[
        \nabla \cdot \nabla u(x,y,z).
        \]
        
        \begin{exercise}
        Show that
        \[
        \nabla \cdot \nabla u(x,y,z) = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} + \frac{\partial^2 u}{\partial z^2}.
        \]
        \end{exercise}
        
        \begin{df}{Laplacian}{laplacian}
        We define the quantity
        \[
        \nabla \cdot \nabla u(x,y,z)
        \]
        to be the \textbf{Laplacian of $u(x,y,z)$} and we often write
        \[
        \Delta \coloneqq \nabla \cdot \nabla
        \]
        and call this the \textbf{Laplacian operator}.
        \end{df}
        
        Intuitively, the Laplacian can be summed up in a few ways. 
        \begin{itemize}
            \item The Laplacian is the \emph{divergence} of the \emph{gradient} of a scalar function.
            \item The Laplacian is the sum of ``curvatures" in each direction.
        \end{itemize}
        
        \begin{ex}{Computing the Laplacian}{compute_laplacian}
        Let us consider the functions
        \[
        f(x,y) = x^2+y^2
        \]
        and
        \[
        g(x,y) = x^2-y^2.
        \]
        Then we can compute the gradients of each function to get
        \begin{align*}
            \nabla f(x,y) &= \begin{bmatrix} 2x \\ 2y \end{bmatrix}\\
            \nabla g(x,y) &= \begin{bmatrix} 2x \\ -2y \end{bmatrix}.
        \end{align*}
        We can then compute the divergence of each of these and find
        \begin{align*}
            \nabla \cdot \nabla f(x,y) &= 2+2 = 4\\
            \nabla \cdot \nabla g(x,y) &= 2-2 =0.
        \end{align*}
        Let us see what these two functions look like to get a bit of an intuitive feel.
        \begin{figure}[H]
            \centering
            \begin{subfigure}[h]{.45\textwidth}
            \includegraphics[width=\textwidth]{Figures_Part_6/pos_laplace.png}
            \caption{A plot of $f(x,y).$}
            \end{subfigure}
            ~
            \begin{subfigure}[h]{.45\textwidth}
            \includegraphics[width=\textwidth]{Figures_Part_6/0_laplace.png}
            \caption{A plot of $g(x,y).$}
            \end{subfigure}
        \end{figure}
        Fundamentally we can see that these two functions are different.  It seems that for $f(x,y)$ we are curving upward in both the $x$ and $y$ direction which is what allows the Laplacian to be positive. However, for $g(x,y)$ one direction curves the opposite direction as the other which cancels out and gives us that the Laplacian is zero.  
        
        It turns out that the Laplacian describes many phenomenon. Two examples would be soap films and temperature flow.
        \end{ex}

        
        We have covered all of the differential calculus of multivariate functions that we need.  That does not mean it won't show up again, but the material won't be new.  We now move onto integration in multiple dimensions.
        
 %% MULTIVARIATE INTEGRATION       \chapter{Integration in Multiple Dimensions}
        
        \section{Integration of Scalar Fields}
        
        \subsubsection{One Dimensional Case}
        In the case of one dimensional functions, we integrated to find the area under a curve.  That is, we were given a function $f(x)$ and asked to find
        \[
        \int_a^b f(x)dx
        \]
        which gave us the \emph{net} area under the curve.  Let's briefly review this with an example.
        
        \begin{ex}{One-Dimensional Integral}
        Let $f(x) = x^2+2$, $a=1,$ and $b=2$. Then we want to find
        \[
        \int_a^b f(x)dx = \int_1^2 x^2+2dx.
        \]
        Then we use the \emph{Fundamental Theorem of Calculus}. So, we find the antiderivative of the integrand and evaluate at the endpoints as follows
        \begin{align*}
            \int_1^2 x^2+2dx &= \left[ \frac{x^3}{3}+2x\right]_1^2\\
            &= \left(\frac{2^3}{3}+2(2)\right) - \left( \frac{1^3}{3}+2(1)\right)\\
            &= \frac{13}{3}.
        \end{align*}
        \end{ex}
        
        \subsubsection{Two Dimensional Case}
        
        Say we are now given a function $f(x,y)$ and bounds on both the $x$ and $y$ by
        $x_0 \leq x \leq x_1$ and $y_0 \leq y \leq y_1$.  We then wish to evaluate
        \[
        \int_{y_0}^{y_1} \int_{x_0}^{x_1} f(x,y)dxdy.
        \]
        You can think of this integral as being the \emph{net} volume under the surface given by $f(x,y)$.  
        
        How do we compute such an integral? The answer is iteratively.  Let's see how we do this with a concrete example.
        
        \begin{ex}{Two-Dimensional Integral}{2d_int}
        Let $f(x,y)=xy$, $x_0=1$, $x_1=2$, $y_0=3$ and $y_1=4$.  So, we want to evaluate
        \[
        \int_{y_0}^{y_1}\int_{x_0}^{x_1} f(x,y)dxdy = \int_3^4 \int_1^2 xy dxdy.
        \]
        The way we do this is by first evaluating the integral with respect to $x$ (holding $y$ constant) and then integrate with respect to $y$ ($x$ will not appear here). So, we integrate from the inside out.  
        
        Let's start by integrating with respect to $x$. We take
        \begin{align*}
            \int_1^2 xy dx &= \left[ \frac{x^2y}{2}\right]_1^2\\
            &= \left( y\frac{2^2}{2}\right) - \left(y\frac{1^2}{2}\right)\\
            &=\frac{3}{2}y.
        \end{align*}
        Now we take this function of $y$, and we integrate this with the bounds we are given.  
        \begin{align*}
            \int_3^4 \frac{3}{2}y dy &= \left[ \frac{3y^2}{4} \right]_3^4\\
            &= \frac{21}{4}.
        \end{align*}
        So we say that
        \[
        \int_3^4 \int_1^2 xy dxdy = \frac{21}{4}.
        \]
        
        Let's walk through the steps again. We did
        \begin{align*}
            \int_{y_0}^{y_1} \int_{x_0}^{x_1} f(x,y)dxdy&= \int_3^4\int_1^2 xy dxdy\\
            &= \int_3^4 \frac{3}{2}ydy \\
            &= \frac{21}{4}.
        \end{align*}
        \end{ex}
        
        \subsubsection{Three Dimensional Case}
        
        Integration here is performed in the same way.  We are given a function $f(x,y,z)$ and bounds on $x$, $y$, and $z$ such as $x_0\leq x \leq x_1$, $y_0\leq y\leq y_1$, and $z_0\leq z \leq z_1$. Then we evaluate
        \[
        \int_{z_0}^{z_1}\int_{y_0}^{y_1}\int_{x_0}^{x_1} f(x,y,z)dxdydz.
        \]
        Let's work through an example.
        
        \begin{ex}{Three-Dimensional Integral}{3d_int}
        Let
        \[
        f(x,y,z)=2x + 8xyz + 3,
        \]
        $x_0 = 0$, $x_1=1$, $y_0=2$, $y_1=3$, $z_0 =4$, $z_1=5$.  Then we want to find
        \[
        \int_{z_0}^{z_1}\int_{y_0}^{y_1}\int_{x_0}^{x_1} f(x,y,z)dxdydz = \int_4^5 \int_2^3 \int_0^1 2x+8xyz+3dxdydz.
        \]
        We do this iteratively.  So we first evaluate the $x$ integral holding the other variables constant for now.
        \begin{align*}
            \int_0^1 2x+8xyz+3 dx &= \left[ x^2 + 4x^2yz+3x\right]_0^1\\
            &= \left( 1^2 + 4(1)^2yz+3(1)\right) - \left( 0^2+4(0)^2yz+3(0)\right)\\
            &= 4yz+4.
        \end{align*}
        We then take this, and integrate with respect to $y$.
        \begin{align*}
            \int_2^3 4yz + 4 dy &= \left[ 2y^2z+4y\right]_2^3\\
            &= (4(3)^2z+4(3))-(4(2)^2z+4(2))\\
            &= 10z+4.
        \end{align*}
        Lastly, we integrate with respect to $z$
        \begin{align*}
            \int_4^5 10z+4 dz &= \left[ 5z^2+4z\right]_4^5\\
            &= (5(5)^2+4(5))-(5(4)^2+4(4))\\
            &=49.
        \end{align*}
        So we say that
        \[
        \int_4^5 \int_2^3 \int_0^1 2x+8xyz+3dxdydz = 49.
        \]
        Again, let's walk through the steps a bit
        \begin{align*}
            \int_{z_0}^{z_1}\int_{y_0}^{y_1}\int_{x_0}^{x_1} f(x,y,z)dxdydz &= \int_4^5 \int_2^3 \int_0^1 2x+8xyz+3dxdydz\\
            &= \int_4^5 \int_3^4 4yz+4dydz\\
            &= \int_4^5 10z+4dz\\
            &= 49.
        \end{align*}
        \end{ex}
        
        \section{Antiderivatives}
        In one variable calculus, we found that there is a relationship between the derivative and the indefinite integral.  In fact, this led us to call the indefinite integral the antiderivative.  This relationship was that
        \[
        \frac{d}{dx}\int f(x)dx = f(x)
        \]
        and
        \[
        \int \frac{df}{dx} = f(x) + C.
        \]
        From this, we realized that the indefinite integral is almost an inverse operation of the derivative.  It's just that in the case where we integrate a derivative, we only determine the function up to an additive constant. 
        
        \subsubsection{Potential Functions}
        The higher dimensional analog happens to be a bit more nuanced but the idea remains the same. Let's say we are given a function $f(x,y,z)$ and we compute, for example,
        \[
        \partialx.
        \]
        The issue now becomes this.  Let's say that we let
        \[
        f(x,y,z) = x+yz.
        \]
        Then we have that
        \[
        \partialx = 1.
        \]
        The terms with just a $y,z$ dependence disappear.  So if we were to try to undo this with an integral, we find that 
        \[
        \int \partialx dx = \int 1 dx = x + g(y,z).
        \]
        That is to say, when we take an indefinite integral a multivariate function with respect to one variable, there could be a function of the residual variables that we cannot determine!
        
        \subsubsection{Integrating the Gradient}
        
        Let's say that we are given $\mathbf{v}(x,y,z)=\nabla f(x,y,z)$ and are asked to find the original function $f(x,y,z)$.  This problem is called finding the \textbf{potential function} for $\mathbf{v}$.  Remember that
        \[
        \nabla f(x,y,z) = \begin{bmatrix} \partialx \\ \partialy \\ \partialz \end{bmatrix}.
        \]
        What we do is the following.
        \begin{enumerate}[1.]
            \item We integrate $\partialx$ with respect to $x$ and determine $f(x,y,z)$ up to adding a function of only $y$ and $z$.  That is we are able to recover what is essentially a third of the potential function $f(x,y,z)$.
            \item We integrate $\partialy$ with respect to $y$ and determine $f(x,y,z)$ up to adding a function of only $x$ and $z$.  
            \item We integrate $\partialz$ with respect to $z$ and determine $f(x,y,z)$ up to adding a function of only $x$ and $z$.
            \item Combine our knowledge from those three integrals we have determined $f(x,y,z)$ up to some additive constant!
        \end{enumerate}
        Let's work through an example.
        
        \begin{ex}{Finding a Potential Function}{find_potential}
        Let's say that we are given the gradient of some function
        \[
        \nabla f(x,y,z) = \begin{bmatrix} y+z \\ x+z \\ x+y \end{bmatrix}.
        \]
        Then we follow the steps above
        \begin{enumerate}[1.]
            \item We integrate $\partialx$ with respect to $x$.  So we have
            \begin{align*}
                \int y+z dx &= xy+xz + g(y,z).
            \end{align*}
            Here, $g(y,z)$ is a function of just $y$ and $z$ that we cannot determine yet.
            \item We integrate $\partialy$ with respect to $y$. So we have
            \begin{align*}
                \int x+z dy &= xy+yz + h(x,z).
            \end{align*}
            \item We integrate $\partialz$ with respect to $z$. So we have
            \begin{align*}
                \int x+y dz &= xz+yz + r(x,y).
            \end{align*}
            \item Now we know that all of these functions should be equal (up to a constant).  That is
            \[
            xy+xz+g(y,z)=xy+yz+h(x,z)=xz+yz+r(x,y).
            \]
            Here, we can see that $g(y,z)=yz$, $h(x,z)=xz$, and $r(x,y)=xy$.  So we have found that 
            \[
            f(x,y,z) = xy + xz + yz + C
            \]
            where the additive constant is there and is not something we can determine without a bit more information.
        \end{enumerate}
        \end{ex}
        
        \subsubsection{Requirements for Potentials}
        
        The main result here is the following.
        
        \begin{prop}{Curl of Gradient is Zero}{curl_of_gradient}
        We have that
        \[
        \nabla \times \nabla f(x,y,z) = \mathbf{0}
        \]
        for all $f(x,y,z)$.
        \end{prop}
        
        Then with a bit more work, one can show this follows.
        
        \begin{thm}{Potential $\iff$ Curl Free}{potential_curl_free}
        Let $\mathbf{v}(x,y,z)$ be a vector field.  Then if 
        \[
        \nabla \times \mathbf{v} = \mathbf{0},
        \]
        then $\mathbf{v}(x,y,z) = \nabla f(x,y,z)$.  That is, a curl-free vector field $\mathbf{v}$ is really the gradient of some scalar function $f(x,y,z)$.  We call this scalar function $f$ the \textbf{potential} for $\mathbf{v}$.
        \end{thm}
        
        This is what allows one to define the \emph{voltage} in electrostatics.  When charges are not moving, we have that the electric field $\mathbf{E}$ satisfies
        \[
        \nabla \times \mathbf{E} = \mathbf{0}
        \]
        and so it follows that
        \[
        \nabla V(x,y,z) = \mathbf{E}
        \]
        where $V$ is the potential.  We often call this electrostatic potential the voltage.
